{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kMJ1IAR4joH",
        "outputId": "cfef33ad-ee5b-4758-9c16-e16b75b20d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=170-6j9q2YmCT7tmwo8tgP3D0jjH489lA\n",
            "To: /content/output.zip\n",
            "100% 43.9M/43.9M [00:00<00:00, 57.3MB/s]\n",
            "Archive:  output.zip\n",
            "  inflating: content/output.csv      \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 170-6j9q2YmCT7tmwo8tgP3D0jjH489lA\n",
        "!unzip output.zip\n",
        "!mv /content/content/output.csv /content/output.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "M-uPZusUYCXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g61FIOQ1DXaf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "import tensorflow as tf\n",
        "from keras.models import model_from_json\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "import keras\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "CnSK-qTKViCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Bey8Bnc1oQN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a680a6-b8e2-41ba-fb8d-dc28d51333c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BDToehJDcGU"
      },
      "outputs": [],
      "source": [
        "def clean_dataset(df):\n",
        "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
        "    df.dropna(inplace=True)\n",
        "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
        "    return df[indices_to_keep]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_So4YTwMDc4L"
      },
      "outputs": [],
      "source": [
        "def y_labels(data):\n",
        "    dict={'Adware':0,'Banker':1 , 'FileInfector':2,'PUA':3,'Riskware':4,'SMS':5,'Trojan':6,'Backdoor':7,'Dropper':8,'NoCategory':9,'Ransomware':10, 'Scareware':11 ,'Spy':12 ,'Zeroday':13,'Benign':14}\n",
        "    for key, value in dict.items():\n",
        "      data[data[9505]==key]=value\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjfcIRPR3sPP"
      },
      "outputs": [],
      "source": [
        "def reduce_size_float(data):\n",
        "  for col in data.columns:\n",
        "    data[col] = data[col].astype(np.int16)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTQaAGNKFKWf"
      },
      "outputs": [],
      "source": [
        "best_features=[]\n",
        "myFile = open(\"best_features.txt\", \"r\")\n",
        "for myLine in myFile:\n",
        "  try:\n",
        "    best_features.append(int(myLine))\n",
        "  except:\n",
        "    continue  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTEYR9QgPJ_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af277876-d7ab-4268-c72b-086efeb0dae8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[58,\n",
              " 51,\n",
              " 59,\n",
              " 38,\n",
              " 49,\n",
              " 3336,\n",
              " 64,\n",
              " 37,\n",
              " 3294,\n",
              " 26,\n",
              " 3053,\n",
              " 36,\n",
              " 52,\n",
              " 67,\n",
              " 129,\n",
              " 3396,\n",
              " 3410,\n",
              " 139,\n",
              " 40,\n",
              " 3343,\n",
              " 27,\n",
              " 144,\n",
              " 3295,\n",
              " 108,\n",
              " 3744,\n",
              " 3411,\n",
              " 61,\n",
              " 57,\n",
              " 3283,\n",
              " 100,\n",
              " 148,\n",
              " 77,\n",
              " 102,\n",
              " 1538,\n",
              " 107,\n",
              " 120,\n",
              " 3285,\n",
              " 2058,\n",
              " 68,\n",
              " 25,\n",
              " 3537,\n",
              " 60,\n",
              " 9333,\n",
              " 3463,\n",
              " 22,\n",
              " 23,\n",
              " 9137,\n",
              " 10,\n",
              " 39,\n",
              " 53,\n",
              " 24,\n",
              " 3444,\n",
              " 2,\n",
              " 5,\n",
              " 3302,\n",
              " 6,\n",
              " 9,\n",
              " 119,\n",
              " 21,\n",
              " 1539,\n",
              " 130,\n",
              " 20,\n",
              " 3054,\n",
              " 8,\n",
              " 3539,\n",
              " 3341,\n",
              " 33,\n",
              " 29,\n",
              " 3327,\n",
              " 3282,\n",
              " 3296,\n",
              " 7,\n",
              " 8414,\n",
              " 56,\n",
              " 105,\n",
              " 8413,\n",
              " 3290,\n",
              " 8388,\n",
              " 32,\n",
              " 3284,\n",
              " 47,\n",
              " 9135,\n",
              " 72,\n",
              " 66,\n",
              " 63,\n",
              " 54,\n",
              " 135,\n",
              " 18,\n",
              " 12,\n",
              " 225,\n",
              " 7173,\n",
              " 3326,\n",
              " 3443,\n",
              " 278,\n",
              " 161,\n",
              " 134,\n",
              " 65,\n",
              " 16,\n",
              " 4339,\n",
              " 41,\n",
              " 3457,\n",
              " 156,\n",
              " 3323,\n",
              " 124,\n",
              " 3374,\n",
              " 405,\n",
              " 3476,\n",
              " 3420,\n",
              " 9136,\n",
              " 43,\n",
              " 220,\n",
              " 153,\n",
              " 9144,\n",
              " 50,\n",
              " 140,\n",
              " 3331,\n",
              " 30,\n",
              " 264,\n",
              " 13,\n",
              " 3332,\n",
              " 14,\n",
              " 3324,\n",
              " 3373,\n",
              " 407,\n",
              " 8385,\n",
              " 443,\n",
              " 3462,\n",
              " 3794,\n",
              " 3288,\n",
              " 8384,\n",
              " 6008,\n",
              " 17,\n",
              " 4829,\n",
              " 4096,\n",
              " 28,\n",
              " 2872,\n",
              " 3397,\n",
              " 3377,\n",
              " 190,\n",
              " 3453,\n",
              " 87,\n",
              " 759,\n",
              " 35,\n",
              " 42,\n",
              " 325,\n",
              " 980,\n",
              " 3304,\n",
              " 296,\n",
              " 151,\n",
              " 104,\n",
              " 154,\n",
              " 3934,\n",
              " 34,\n",
              " 74,\n",
              " 4038,\n",
              " 62,\n",
              " 4828,\n",
              " 4039,\n",
              " 15,\n",
              " 82,\n",
              " 856,\n",
              " 11,\n",
              " 3287,\n",
              " 99,\n",
              " 142,\n",
              " 282,\n",
              " 527,\n",
              " 200,\n",
              " 6312,\n",
              " 3305,\n",
              " 3330,\n",
              " 31,\n",
              " 3640,\n",
              " 19,\n",
              " 3299,\n",
              " 5182,\n",
              " 112,\n",
              " 3407,\n",
              " 106,\n",
              " 3301,\n",
              " 98,\n",
              " 131,\n",
              " 78,\n",
              " 3298,\n",
              " 4448,\n",
              " 137,\n",
              " 88,\n",
              " 95,\n",
              " 75,\n",
              " 80,\n",
              " 165,\n",
              " 79,\n",
              " 336,\n",
              " 109,\n",
              " 564,\n",
              " 84,\n",
              " 76,\n",
              " 92,\n",
              " 4787,\n",
              " 3,\n",
              " 246,\n",
              " 113,\n",
              " 160,\n",
              " 44,\n",
              " 4621,\n",
              " 4,\n",
              " 85,\n",
              " 3293,\n",
              " 83,\n",
              " 3329,\n",
              " 86,\n",
              " 3291,\n",
              " 126,\n",
              " 3460,\n",
              " 3367,\n",
              " 3286,\n",
              " 163,\n",
              " 218,\n",
              " 45,\n",
              " 121,\n",
              " 1605,\n",
              " 3361,\n",
              " 123,\n",
              " 110,\n",
              " 3694,\n",
              " 3450,\n",
              " 9139,\n",
              " 46,\n",
              " 3564,\n",
              " 132,\n",
              " 81,\n",
              " 3449,\n",
              " 3451,\n",
              " 73,\n",
              " 96,\n",
              " 3478,\n",
              " 128,\n",
              " 3799,\n",
              " 3303,\n",
              " 3364,\n",
              " 5336,\n",
              " 9138,\n",
              " 9159,\n",
              " 350,\n",
              " 70,\n",
              " 3312,\n",
              " 136,\n",
              " 168,\n",
              " 9423,\n",
              " 3412,\n",
              " 4788,\n",
              " 164,\n",
              " 9146,\n",
              " 1308,\n",
              " 9143,\n",
              " 3479,\n",
              " 232,\n",
              " 201,\n",
              " 3452,\n",
              " 8427,\n",
              " 9225,\n",
              " 7711,\n",
              " 473,\n",
              " 3292,\n",
              " 388,\n",
              " 556,\n",
              " 234,\n",
              " 485,\n",
              " 3378,\n",
              " 9152,\n",
              " 48,\n",
              " 445,\n",
              " 138,\n",
              " 3337,\n",
              " 141,\n",
              " 3363,\n",
              " 1105,\n",
              " 3395,\n",
              " 93,\n",
              " 69,\n",
              " 115,\n",
              " 188,\n",
              " 3480,\n",
              " 3368,\n",
              " 621,\n",
              " 125,\n",
              " 3540,\n",
              " 529,\n",
              " 8869,\n",
              " 3784,\n",
              " 94,\n",
              " 133,\n",
              " 3358,\n",
              " 3860,\n",
              " 471,\n",
              " 8389,\n",
              " 3726,\n",
              " 1104,\n",
              " 7513,\n",
              " 3883,\n",
              " 213,\n",
              " 6150,\n",
              " 3365,\n",
              " 209,\n",
              " 3527,\n",
              " 6149,\n",
              " 3345,\n",
              " 210,\n",
              " 3366,\n",
              " 211,\n",
              " 212,\n",
              " 3474,\n",
              " 101,\n",
              " 6806,\n",
              " 3313,\n",
              " 118,\n",
              " 3498,\n",
              " 484,\n",
              " 358,\n",
              " 8465,\n",
              " 71,\n",
              " 89,\n",
              " 3442,\n",
              " 297,\n",
              " 3342,\n",
              " 91,\n",
              " 560,\n",
              " 6151,\n",
              " 3710,\n",
              " 157,\n",
              " 3800,\n",
              " 9148,\n",
              " 3477,\n",
              " 155,\n",
              " 3309,\n",
              " 205,\n",
              " 239,\n",
              " 103,\n",
              " 3446,\n",
              " 3467,\n",
              " 323,\n",
              " 3448,\n",
              " 3306,\n",
              " 3421,\n",
              " 3297,\n",
              " 249,\n",
              " 9175,\n",
              " 339,\n",
              " 3308,\n",
              " 5029,\n",
              " 9142,\n",
              " 561,\n",
              " 3310,\n",
              " 849,\n",
              " 425,\n",
              " 3300,\n",
              " 3314,\n",
              " 269,\n",
              " 3311,\n",
              " 162,\n",
              " 97,\n",
              " 9147,\n",
              " 372,\n",
              " 440,\n",
              " 270,\n",
              " 3651,\n",
              " 3454,\n",
              " 285,\n",
              " 3422,\n",
              " 5200,\n",
              " 3425,\n",
              " 3447,\n",
              " 173,\n",
              " 3838,\n",
              " 3445,\n",
              " 5818,\n",
              " 3631,\n",
              " 169,\n",
              " 492,\n",
              " 8386,\n",
              " 3419,\n",
              " 367,\n",
              " 3630,\n",
              " 510,\n",
              " 7174,\n",
              " 3571,\n",
              " 1246,\n",
              " 3521,\n",
              " 8453,\n",
              " 8390,\n",
              " 6428,\n",
              " 122,\n",
              " 3573,\n",
              " 3322,\n",
              " 8426,\n",
              " 214,\n",
              " 150,\n",
              " 326,\n",
              " 307,\n",
              " 8387,\n",
              " 3742,\n",
              " 3584,\n",
              " 3511,\n",
              " 295,\n",
              " 3482,\n",
              " 600,\n",
              " 3433,\n",
              " 3321,\n",
              " 2557,\n",
              " 4411,\n",
              " 9154,\n",
              " 3553,\n",
              " 197,\n",
              " 3434,\n",
              " 111,\n",
              " 3289,\n",
              " 3456,\n",
              " 310,\n",
              " 179,\n",
              " 2915,\n",
              " 3346,\n",
              " 3574,\n",
              " 298,\n",
              " 1962,\n",
              " 674,\n",
              " 3578,\n",
              " 191,\n",
              " 8542,\n",
              " 3092,\n",
              " 9155,\n",
              " 2914,\n",
              " 3750,\n",
              " 247,\n",
              " 534,\n",
              " 9430,\n",
              " 90,\n",
              " 720,\n",
              " 3455,\n",
              " 738,\n",
              " 3840,\n",
              " 1646,\n",
              " 3437,\n",
              " 3440,\n",
              " 2598,\n",
              " 4396,\n",
              " 3618,\n",
              " 3436,\n",
              " 771,\n",
              " 595,\n",
              " 3435,\n",
              " 3339,\n",
              " 3438,\n",
              " 3459,\n",
              " 365,\n",
              " 516,\n",
              " 3590,\n",
              " 3559,\n",
              " 3637,\n",
              " 149,\n",
              " 3426,\n",
              " 368,\n",
              " 333,\n",
              " 259,\n",
              " 3686,\n",
              " 9165,\n",
              " 604,\n",
              " 3509,\n",
              " 3852,\n",
              " 3429,\n",
              " 3359,\n",
              " 1029,\n",
              " 355,\n",
              " 4105,\n",
              " 8411,\n",
              " 653,\n",
              " 392,\n",
              " 254,\n",
              " 3753,\n",
              " 2247,\n",
              " 385,\n",
              " 515,\n",
              " 302,\n",
              " 8410,\n",
              " 8323,\n",
              " 691,\n",
              " 3719,\n",
              " 306,\n",
              " 8867,\n",
              " 3580,\n",
              " 4046,\n",
              " 305,\n",
              " 8579,\n",
              " 243,\n",
              " 3513,\n",
              " 3338,\n",
              " 3874,\n",
              " 199,\n",
              " 4034,\n",
              " 3579,\n",
              " 348,\n",
              " 3388,\n",
              " 4112,\n",
              " 9126,\n",
              " 517,\n",
              " 4340,\n",
              " 3334,\n",
              " 742,\n",
              " 1775,\n",
              " 3483,\n",
              " 222,\n",
              " 5477,\n",
              " 755,\n",
              " 8464,\n",
              " 3376,\n",
              " 2538,\n",
              " 4356,\n",
              " 3623,\n",
              " 3389,\n",
              " 9498,\n",
              " 3862,\n",
              " 4568,\n",
              " 9183,\n",
              " 1577,\n",
              " 330,\n",
              " 238,\n",
              " 3508,\n",
              " 8764,\n",
              " 1649,\n",
              " 3340,\n",
              " 3645,\n",
              " 5611,\n",
              " 9497,\n",
              " 4130,\n",
              " 757,\n",
              " 9127,\n",
              " 3626,\n",
              " 335,\n",
              " 3575,\n",
              " 3190,\n",
              " 317,\n",
              " 9149,\n",
              " 2883,\n",
              " 4040,\n",
              " 8765,\n",
              " 8789,\n",
              " 3325,\n",
              " 780,\n",
              " 1336,\n",
              " 3605,\n",
              " 4154,\n",
              " 580,\n",
              " 3234,\n",
              " 3316,\n",
              " 3620,\n",
              " 620,\n",
              " 3625,\n",
              " 8412,\n",
              " 3756,\n",
              " 722,\n",
              " 3335,\n",
              " 196,\n",
              " 8407,\n",
              " 955,\n",
              " 2539,\n",
              " 687,\n",
              " 152,\n",
              " 3849,\n",
              " 277,\n",
              " 3375,\n",
              " 3863,\n",
              " 4440,\n",
              " 8098,\n",
              " 8303,\n",
              " 6293,\n",
              " 3576,\n",
              " 193,\n",
              " 8440,\n",
              " 9121,\n",
              " 8423,\n",
              " 3666,\n",
              " 4648,\n",
              " 4155,\n",
              " 9130,\n",
              " 8459,\n",
              " 3432,\n",
              " 9153,\n",
              " 4153,\n",
              " 3557,\n",
              " 3555,\n",
              " 9125,\n",
              " 3780,\n",
              " 4156,\n",
              " 3781,\n",
              " 3664,\n",
              " 3526,\n",
              " 2817,\n",
              " 8617,\n",
              " 3427,\n",
              " 260,\n",
              " 2003,\n",
              " 690,\n",
              " 2627,\n",
              " 251,\n",
              " 248,\n",
              " 2986,\n",
              " 5262,\n",
              " 9500,\n",
              " 3916,\n",
              " 114,\n",
              " 9150,\n",
              " 3616,\n",
              " 2659,\n",
              " 2537,\n",
              " 2836,\n",
              " 6120,\n",
              " 8406,\n",
              " 9499,\n",
              " 2472,\n",
              " 3779,\n",
              " 3461,\n",
              " 3372,\n",
              " 8863,\n",
              " 4151,\n",
              " 4587,\n",
              " 3089,\n",
              " 3360,\n",
              " 2564,\n",
              " 696,\n",
              " 3510,\n",
              " 3556,\n",
              " 1617,\n",
              " 434,\n",
              " 1618,\n",
              " 9457,\n",
              " 271,\n",
              " 3615,\n",
              " 1059,\n",
              " 770,\n",
              " 5493,\n",
              " 1676,\n",
              " 9030,\n",
              " 9496,\n",
              " 1773,\n",
              " 315,\n",
              " 2725,\n",
              " 3344,\n",
              " 3084,\n",
              " 9151,\n",
              " 3512,\n",
              " 377,\n",
              " 6441,\n",
              " 9215,\n",
              " 1106,\n",
              " 820,\n",
              " 9190,\n",
              " 3624,\n",
              " 8864,\n",
              " 9124,\n",
              " 4652,\n",
              " 8861,\n",
              " 6439,\n",
              " 3528,\n",
              " 3708,\n",
              " 3755,\n",
              " 6442,\n",
              " 2670,\n",
              " 3610,\n",
              " 331,\n",
              " 2080,\n",
              " 3907,\n",
              " 9123,\n",
              " 8790,\n",
              " 2726,\n",
              " 4095,\n",
              " 314,\n",
              " 3471,\n",
              " 116,\n",
              " 4159,\n",
              " 4407,\n",
              " 3604,\n",
              " 5064,\n",
              " 4341,\n",
              " 3525,\n",
              " 273,\n",
              " 5531,\n",
              " 8392,\n",
              " 4141,\n",
              " 4585,\n",
              " 3949,\n",
              " 3577,\n",
              " 1906,\n",
              " 2476,\n",
              " 724,\n",
              " 4590,\n",
              " 3639,\n",
              " 279,\n",
              " 4177,\n",
              " 2578,\n",
              " 3711,\n",
              " 1777,\n",
              " 4097,\n",
              " 1957,\n",
              " 6599,\n",
              " 6367,\n",
              " 9128,\n",
              " 9129,\n",
              " 8862,\n",
              " 3754,\n",
              " 747,\n",
              " 6382,\n",
              " 3233,\n",
              " 1961,\n",
              " 8422,\n",
              " 261,\n",
              " 3881,\n",
              " 3307,\n",
              " 2343,\n",
              " 8461,\n",
              " 244,\n",
              " 8543,\n",
              " 9166,\n",
              " 4315,\n",
              " 3371,\n",
              " 228,\n",
              " 3044,\n",
              " 4591,\n",
              " 2478,\n",
              " 1207,\n",
              " 3787,\n",
              " 253,\n",
              " 217,\n",
              " 748,\n",
              " 3362,\n",
              " 3743,\n",
              " 4129,\n",
              " 3747,\n",
              " 221,\n",
              " 9272,\n",
              " 8896,\n",
              " 3355,\n",
              " 241,\n",
              " 351,\n",
              " 5234,\n",
              " 3593,\n",
              " 4592,\n",
              " 4589,\n",
              " 2523,\n",
              " 1633,\n",
              " 4024,\n",
              " 6440,\n",
              " 8651,\n",
              " 3648,\n",
              " 216,\n",
              " 8420,\n",
              " 2664,\n",
              " 9058,\n",
              " 744,\n",
              " 383,\n",
              " 2996,\n",
              " 2888,\n",
              " 3786,\n",
              " 4253,\n",
              " 9173,\n",
              " 4530,\n",
              " 3697,\n",
              " 414,\n",
              " 8596,\n",
              " 2941,\n",
              " 772,\n",
              " 9122,\n",
              " 287,\n",
              " 8781,\n",
              " 9329,\n",
              " 8463,\n",
              " 8615,\n",
              " 8623,\n",
              " 4515,\n",
              " 9068,\n",
              " 186,\n",
              " 3652,\n",
              " 229,\n",
              " 9502,\n",
              " 3417,\n",
              " 3718,\n",
              " 2595,\n",
              " 127,\n",
              " 6375,\n",
              " 8456,\n",
              " 4586,\n",
              " 8594,\n",
              " 4593,\n",
              " 3737,\n",
              " 8421,\n",
              " 6366,\n",
              " 5568,\n",
              " 2605,\n",
              " 4853,\n",
              " 1241,\n",
              " 3458,\n",
              " 928,\n",
              " 390,\n",
              " 692,\n",
              " 9374,\n",
              " 9145,\n",
              " 415,\n",
              " 1135,\n",
              " 167,\n",
              " 857,\n",
              " 1859,\n",
              " 8454,\n",
              " 3390,\n",
              " 4634,\n",
              " 3729,\n",
              " 4588,\n",
              " 6368,\n",
              " 230,\n",
              " 3418,\n",
              " 143,\n",
              " 503,\n",
              " 1645,\n",
              " 3317,\n",
              " 4077,\n",
              " 8796,\n",
              " 6387,\n",
              " 4013,\n",
              " 3386,\n",
              " 3095,\n",
              " 3644,\n",
              " 353,\n",
              " 3572,\n",
              " 309,\n",
              " 419,\n",
              " 4620,\n",
              " 420,\n",
              " 3709,\n",
              " 8659,\n",
              " 3504,\n",
              " 1655,\n",
              " 5989,\n",
              " 7270,\n",
              " 6438,\n",
              " 622,\n",
              " 3601,\n",
              " 4644,\n",
              " 634,\n",
              " 4447,\n",
              " 1188,\n",
              " 458,\n",
              " 2755,\n",
              " 453,\n",
              " 2690,\n",
              " 272,\n",
              " 3695,\n",
              " 3608,\n",
              " 9185,\n",
              " 308,\n",
              " 2034,\n",
              " 3687,\n",
              " 3730,\n",
              " 6370,\n",
              " 8447,\n",
              " 4514,\n",
              " 8425,\n",
              " 3595,\n",
              " 413,\n",
              " 4456,\n",
              " 673,\n",
              " 740,\n",
              " 328,\n",
              " 587,\n",
              " 814,\n",
              " 9036,\n",
              " 1203,\n",
              " 2791,\n",
              " 3414,\n",
              " 4516,\n",
              " 5241,\n",
              " 5296,\n",
              " 6383,\n",
              " 1974,\n",
              " 3831,\n",
              " 6298,\n",
              " 2632,\n",
              " 8747,\n",
              " 8865,\n",
              " 5983,\n",
              " 9450,\n",
              " 3955,\n",
              " 2536,\n",
              " 8450,\n",
              " 3413,\n",
              " 3041,\n",
              " 8746,\n",
              " 8745,\n",
              " 3690,\n",
              " 6348,\n",
              " 4758,\n",
              " 3734,\n",
              " 4359,\n",
              " 3409,\n",
              " 6501,\n",
              " 2555,\n",
              " 8797,\n",
              " 4845,\n",
              " 1083,\n",
              " 1641,\n",
              " 2177,\n",
              " 2816,\n",
              " 6385,\n",
              " 3029,\n",
              " 1468,\n",
              " 8925,\n",
              " 263,\n",
              " 8424,\n",
              " 4496,\n",
              " 9271,\n",
              " 5842,\n",
              " 8604,\n",
              " 3972,\n",
              " 4337,\n",
              " 2676,\n",
              " 1639,\n",
              " 9268,\n",
              " 4254,\n",
              " 3628,\n",
              " 591,\n",
              " 9039,\n",
              " 4158,\n",
              " 8462,\n",
              " 274,\n",
              " 6147,\n",
              " 3472,\n",
              " 380,\n",
              " 3627,\n",
              " 8165,\n",
              " 3641,\n",
              " 9059,\n",
              " 9156,\n",
              " 8866,\n",
              " 379,\n",
              " 8593,\n",
              " 9501,\n",
              " 1374,\n",
              " 2470,\n",
              " 5686,\n",
              " 9204,\n",
              " 2698,\n",
              " 3394,\n",
              " 508,\n",
              " 9091,\n",
              " 8607,\n",
              " 3732,\n",
              " 9119,\n",
              " 790,\n",
              " 6379,\n",
              " 2897,\n",
              " 8405,\n",
              " 8166,\n",
              " 2637,\n",
              " 8588,\n",
              " 4171,\n",
              " 2699,\n",
              " 6386,\n",
              " 2804,\n",
              " 429,\n",
              " 568,\n",
              " 3797,\n",
              " 8664,\n",
              " 9456,\n",
              " 3612,\n",
              " 5718,\n",
              " 2638,\n",
              " 2980,\n",
              " 3696,\n",
              " 3931,\n",
              " 4175,\n",
              " 3951,\n",
              " 1640,\n",
              " 961,\n",
              " 4092,\n",
              " 159,\n",
              " 976,\n",
              " 499,\n",
              " 3807,\n",
              " 416,\n",
              " 4408,\n",
              " 382,\n",
              " 5128,\n",
              " 3232,\n",
              " 3621,\n",
              " 2759,\n",
              " 4943,\n",
              " 8432,\n",
              " 2556,\n",
              " 9184,\n",
              " 3900,\n",
              " 4168,\n",
              " 7273,\n",
              " 3027,\n",
              " 3851,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "best_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TZCl0knPUFJ",
        "outputId": "9ed5072d-a60e-4cd8-f8dd-6e251e76a64a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1238"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(best_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGI-Ddv2PXGq"
      },
      "outputs": [],
      "source": [
        "best_features.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xpm-Re0bPaLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97db1f1-be21-42a2-d37d-c3511c8126a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 76,\n",
              " 77,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 85,\n",
              " 86,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99,\n",
              " 100,\n",
              " 101,\n",
              " 102,\n",
              " 103,\n",
              " 104,\n",
              " 105,\n",
              " 106,\n",
              " 107,\n",
              " 108,\n",
              " 109,\n",
              " 110,\n",
              " 111,\n",
              " 112,\n",
              " 113,\n",
              " 114,\n",
              " 115,\n",
              " 116,\n",
              " 118,\n",
              " 119,\n",
              " 120,\n",
              " 121,\n",
              " 122,\n",
              " 123,\n",
              " 124,\n",
              " 125,\n",
              " 126,\n",
              " 127,\n",
              " 128,\n",
              " 129,\n",
              " 130,\n",
              " 131,\n",
              " 132,\n",
              " 133,\n",
              " 134,\n",
              " 135,\n",
              " 136,\n",
              " 137,\n",
              " 138,\n",
              " 139,\n",
              " 140,\n",
              " 141,\n",
              " 142,\n",
              " 143,\n",
              " 144,\n",
              " 146,\n",
              " 148,\n",
              " 149,\n",
              " 150,\n",
              " 151,\n",
              " 152,\n",
              " 153,\n",
              " 154,\n",
              " 155,\n",
              " 156,\n",
              " 157,\n",
              " 159,\n",
              " 160,\n",
              " 161,\n",
              " 162,\n",
              " 163,\n",
              " 164,\n",
              " 165,\n",
              " 167,\n",
              " 168,\n",
              " 169,\n",
              " 173,\n",
              " 179,\n",
              " 180,\n",
              " 185,\n",
              " 186,\n",
              " 188,\n",
              " 190,\n",
              " 191,\n",
              " 193,\n",
              " 196,\n",
              " 197,\n",
              " 199,\n",
              " 200,\n",
              " 201,\n",
              " 203,\n",
              " 205,\n",
              " 209,\n",
              " 210,\n",
              " 211,\n",
              " 212,\n",
              " 213,\n",
              " 214,\n",
              " 216,\n",
              " 217,\n",
              " 218,\n",
              " 220,\n",
              " 221,\n",
              " 222,\n",
              " 225,\n",
              " 228,\n",
              " 229,\n",
              " 230,\n",
              " 232,\n",
              " 234,\n",
              " 237,\n",
              " 238,\n",
              " 239,\n",
              " 241,\n",
              " 242,\n",
              " 243,\n",
              " 244,\n",
              " 245,\n",
              " 246,\n",
              " 247,\n",
              " 248,\n",
              " 249,\n",
              " 250,\n",
              " 251,\n",
              " 253,\n",
              " 254,\n",
              " 255,\n",
              " 259,\n",
              " 260,\n",
              " 261,\n",
              " 263,\n",
              " 264,\n",
              " 269,\n",
              " 270,\n",
              " 271,\n",
              " 272,\n",
              " 273,\n",
              " 274,\n",
              " 275,\n",
              " 277,\n",
              " 278,\n",
              " 279,\n",
              " 282,\n",
              " 284,\n",
              " 285,\n",
              " 287,\n",
              " 295,\n",
              " 296,\n",
              " 297,\n",
              " 298,\n",
              " 302,\n",
              " 305,\n",
              " 306,\n",
              " 307,\n",
              " 308,\n",
              " 309,\n",
              " 310,\n",
              " 313,\n",
              " 314,\n",
              " 315,\n",
              " 317,\n",
              " 323,\n",
              " 325,\n",
              " 326,\n",
              " 328,\n",
              " 329,\n",
              " 330,\n",
              " 331,\n",
              " 333,\n",
              " 335,\n",
              " 336,\n",
              " 339,\n",
              " 348,\n",
              " 350,\n",
              " 351,\n",
              " 353,\n",
              " 355,\n",
              " 358,\n",
              " 365,\n",
              " 367,\n",
              " 368,\n",
              " 372,\n",
              " 377,\n",
              " 379,\n",
              " 380,\n",
              " 382,\n",
              " 383,\n",
              " 385,\n",
              " 388,\n",
              " 390,\n",
              " 392,\n",
              " 405,\n",
              " 407,\n",
              " 413,\n",
              " 414,\n",
              " 415,\n",
              " 416,\n",
              " 417,\n",
              " 418,\n",
              " 419,\n",
              " 420,\n",
              " 425,\n",
              " 427,\n",
              " 429,\n",
              " 434,\n",
              " 440,\n",
              " 441,\n",
              " 443,\n",
              " 445,\n",
              " 452,\n",
              " 453,\n",
              " 454,\n",
              " 455,\n",
              " 458,\n",
              " 471,\n",
              " 473,\n",
              " 479,\n",
              " 484,\n",
              " 485,\n",
              " 492,\n",
              " 499,\n",
              " 503,\n",
              " 508,\n",
              " 510,\n",
              " 515,\n",
              " 516,\n",
              " 517,\n",
              " 527,\n",
              " 529,\n",
              " 534,\n",
              " 536,\n",
              " 556,\n",
              " 560,\n",
              " 561,\n",
              " 564,\n",
              " 567,\n",
              " 568,\n",
              " 569,\n",
              " 570,\n",
              " 580,\n",
              " 587,\n",
              " 591,\n",
              " 595,\n",
              " 600,\n",
              " 601,\n",
              " 604,\n",
              " 620,\n",
              " 621,\n",
              " 622,\n",
              " 623,\n",
              " 628,\n",
              " 629,\n",
              " 630,\n",
              " 632,\n",
              " 634,\n",
              " 636,\n",
              " 653,\n",
              " 673,\n",
              " 674,\n",
              " 676,\n",
              " 680,\n",
              " 687,\n",
              " 690,\n",
              " 691,\n",
              " 692,\n",
              " 696,\n",
              " 708,\n",
              " 719,\n",
              " 720,\n",
              " 722,\n",
              " 724,\n",
              " 737,\n",
              " 738,\n",
              " 740,\n",
              " 742,\n",
              " 743,\n",
              " 744,\n",
              " 745,\n",
              " 747,\n",
              " 748,\n",
              " 755,\n",
              " 757,\n",
              " 759,\n",
              " 770,\n",
              " 771,\n",
              " 772,\n",
              " 775,\n",
              " 776,\n",
              " 780,\n",
              " 782,\n",
              " 790,\n",
              " 810,\n",
              " 814,\n",
              " 820,\n",
              " 838,\n",
              " 849,\n",
              " 855,\n",
              " 856,\n",
              " 857,\n",
              " 872,\n",
              " 874,\n",
              " 875,\n",
              " 877,\n",
              " 880,\n",
              " 928,\n",
              " 955,\n",
              " 961,\n",
              " 962,\n",
              " 963,\n",
              " 976,\n",
              " 980,\n",
              " 1022,\n",
              " 1023,\n",
              " 1024,\n",
              " 1029,\n",
              " 1059,\n",
              " 1073,\n",
              " 1083,\n",
              " 1104,\n",
              " 1105,\n",
              " 1106,\n",
              " 1127,\n",
              " 1135,\n",
              " 1188,\n",
              " 1190,\n",
              " 1203,\n",
              " 1204,\n",
              " 1207,\n",
              " 1209,\n",
              " 1241,\n",
              " 1246,\n",
              " 1250,\n",
              " 1308,\n",
              " 1335,\n",
              " 1336,\n",
              " 1374,\n",
              " 1441,\n",
              " 1455,\n",
              " 1468,\n",
              " 1538,\n",
              " 1539,\n",
              " 1577,\n",
              " 1601,\n",
              " 1605,\n",
              " 1608,\n",
              " 1609,\n",
              " 1617,\n",
              " 1618,\n",
              " 1633,\n",
              " 1639,\n",
              " 1640,\n",
              " 1641,\n",
              " 1644,\n",
              " 1645,\n",
              " 1646,\n",
              " 1649,\n",
              " 1655,\n",
              " 1676,\n",
              " 1729,\n",
              " 1730,\n",
              " 1773,\n",
              " 1775,\n",
              " 1777,\n",
              " 1818,\n",
              " 1859,\n",
              " 1880,\n",
              " 1906,\n",
              " 1957,\n",
              " 1961,\n",
              " 1962,\n",
              " 1974,\n",
              " 1982,\n",
              " 2001,\n",
              " 2003,\n",
              " 2004,\n",
              " 2034,\n",
              " 2058,\n",
              " 2080,\n",
              " 2177,\n",
              " 2182,\n",
              " 2218,\n",
              " 2247,\n",
              " 2257,\n",
              " 2343,\n",
              " 2470,\n",
              " 2472,\n",
              " 2476,\n",
              " 2478,\n",
              " 2523,\n",
              " 2533,\n",
              " 2536,\n",
              " 2537,\n",
              " 2538,\n",
              " 2539,\n",
              " 2548,\n",
              " 2555,\n",
              " 2556,\n",
              " 2557,\n",
              " 2558,\n",
              " 2562,\n",
              " 2563,\n",
              " 2564,\n",
              " 2572,\n",
              " 2578,\n",
              " 2581,\n",
              " 2595,\n",
              " 2598,\n",
              " 2605,\n",
              " 2612,\n",
              " 2624,\n",
              " 2627,\n",
              " 2632,\n",
              " 2636,\n",
              " 2637,\n",
              " 2638,\n",
              " 2645,\n",
              " 2652,\n",
              " 2659,\n",
              " 2663,\n",
              " 2664,\n",
              " 2670,\n",
              " 2672,\n",
              " 2673,\n",
              " 2676,\n",
              " 2678,\n",
              " 2681,\n",
              " 2690,\n",
              " 2697,\n",
              " 2698,\n",
              " 2699,\n",
              " 2706,\n",
              " 2725,\n",
              " 2726,\n",
              " 2735,\n",
              " 2740,\n",
              " 2752,\n",
              " 2755,\n",
              " 2759,\n",
              " 2791,\n",
              " 2795,\n",
              " 2799,\n",
              " 2804,\n",
              " 2816,\n",
              " 2817,\n",
              " 2818,\n",
              " 2822,\n",
              " 2825,\n",
              " 2829,\n",
              " 2836,\n",
              " 2840,\n",
              " 2841,\n",
              " 2851,\n",
              " 2872,\n",
              " 2883,\n",
              " 2888,\n",
              " 2897,\n",
              " 2914,\n",
              " 2915,\n",
              " 2941,\n",
              " 2980,\n",
              " 2986,\n",
              " 2994,\n",
              " 2996,\n",
              " 3027,\n",
              " 3029,\n",
              " 3041,\n",
              " 3044,\n",
              " 3047,\n",
              " 3053,\n",
              " 3054,\n",
              " 3072,\n",
              " 3084,\n",
              " 3089,\n",
              " 3092,\n",
              " 3094,\n",
              " 3095,\n",
              " 3111,\n",
              " 3190,\n",
              " 3231,\n",
              " 3232,\n",
              " 3233,\n",
              " 3234,\n",
              " 3240,\n",
              " 3263,\n",
              " 3282,\n",
              " 3283,\n",
              " 3284,\n",
              " 3285,\n",
              " 3286,\n",
              " 3287,\n",
              " 3288,\n",
              " 3289,\n",
              " 3290,\n",
              " 3291,\n",
              " 3292,\n",
              " 3293,\n",
              " 3294,\n",
              " 3295,\n",
              " 3296,\n",
              " 3297,\n",
              " 3298,\n",
              " 3299,\n",
              " 3300,\n",
              " 3301,\n",
              " 3302,\n",
              " 3303,\n",
              " 3304,\n",
              " 3305,\n",
              " 3306,\n",
              " 3307,\n",
              " 3308,\n",
              " 3309,\n",
              " 3310,\n",
              " 3311,\n",
              " 3312,\n",
              " 3313,\n",
              " 3314,\n",
              " 3316,\n",
              " 3317,\n",
              " 3321,\n",
              " 3322,\n",
              " 3323,\n",
              " 3324,\n",
              " 3325,\n",
              " 3326,\n",
              " 3327,\n",
              " 3329,\n",
              " 3330,\n",
              " 3331,\n",
              " 3332,\n",
              " 3334,\n",
              " 3335,\n",
              " 3336,\n",
              " 3337,\n",
              " 3338,\n",
              " 3339,\n",
              " 3340,\n",
              " 3341,\n",
              " 3342,\n",
              " 3343,\n",
              " 3344,\n",
              " 3345,\n",
              " 3346,\n",
              " 3355,\n",
              " 3357,\n",
              " 3358,\n",
              " 3359,\n",
              " 3360,\n",
              " 3361,\n",
              " 3362,\n",
              " 3363,\n",
              " 3364,\n",
              " 3365,\n",
              " 3366,\n",
              " 3367,\n",
              " 3368,\n",
              " 3370,\n",
              " 3371,\n",
              " 3372,\n",
              " 3373,\n",
              " 3374,\n",
              " 3375,\n",
              " 3376,\n",
              " 3377,\n",
              " 3378,\n",
              " 3386,\n",
              " 3388,\n",
              " 3389,\n",
              " 3390,\n",
              " 3394,\n",
              " 3395,\n",
              " 3396,\n",
              " 3397,\n",
              " 3407,\n",
              " 3409,\n",
              " 3410,\n",
              " 3411,\n",
              " 3412,\n",
              " 3413,\n",
              " 3414,\n",
              " 3417,\n",
              " 3418,\n",
              " 3419,\n",
              " 3420,\n",
              " 3421,\n",
              " 3422,\n",
              " 3425,\n",
              " 3426,\n",
              " 3427,\n",
              " 3429,\n",
              " 3432,\n",
              " 3433,\n",
              " 3434,\n",
              " 3435,\n",
              " 3436,\n",
              " 3437,\n",
              " 3438,\n",
              " 3440,\n",
              " 3442,\n",
              " 3443,\n",
              " 3444,\n",
              " 3445,\n",
              " 3446,\n",
              " 3447,\n",
              " 3448,\n",
              " 3449,\n",
              " 3450,\n",
              " 3451,\n",
              " 3452,\n",
              " 3453,\n",
              " 3454,\n",
              " 3455,\n",
              " 3456,\n",
              " 3457,\n",
              " 3458,\n",
              " 3459,\n",
              " 3460,\n",
              " 3461,\n",
              " 3462,\n",
              " 3463,\n",
              " 3467,\n",
              " 3471,\n",
              " 3472,\n",
              " 3474,\n",
              " 3476,\n",
              " 3477,\n",
              " 3478,\n",
              " 3479,\n",
              " 3480,\n",
              " 3482,\n",
              " 3483,\n",
              " 3484,\n",
              " 3498,\n",
              " 3504,\n",
              " 3508,\n",
              " 3509,\n",
              " 3510,\n",
              " 3511,\n",
              " 3512,\n",
              " 3513,\n",
              " 3521,\n",
              " 3525,\n",
              " 3526,\n",
              " 3527,\n",
              " 3528,\n",
              " 3537,\n",
              " 3539,\n",
              " 3540,\n",
              " 3553,\n",
              " 3555,\n",
              " 3556,\n",
              " 3557,\n",
              " 3559,\n",
              " 3564,\n",
              " 3565,\n",
              " 3566,\n",
              " 3570,\n",
              " 3571,\n",
              " 3572,\n",
              " 3573,\n",
              " 3574,\n",
              " 3575,\n",
              " 3576,\n",
              " 3577,\n",
              " 3578,\n",
              " 3579,\n",
              " 3580,\n",
              " 3584,\n",
              " 3590,\n",
              " 3593,\n",
              " 3595,\n",
              " 3601,\n",
              " 3604,\n",
              " 3605,\n",
              " 3608,\n",
              " 3609,\n",
              " 3610,\n",
              " 3611,\n",
              " 3612,\n",
              " 3615,\n",
              " 3616,\n",
              " 3618,\n",
              " 3620,\n",
              " 3621,\n",
              " 3622,\n",
              " 3623,\n",
              " 3624,\n",
              " 3625,\n",
              " 3626,\n",
              " 3627,\n",
              " 3628,\n",
              " 3630,\n",
              " 3631,\n",
              " 3637,\n",
              " 3639,\n",
              " 3640,\n",
              " 3641,\n",
              " 3644,\n",
              " 3645,\n",
              " 3648,\n",
              " 3651,\n",
              " 3652,\n",
              " 3655,\n",
              " 3664,\n",
              " 3666,\n",
              " 3686,\n",
              " 3687,\n",
              " 3688,\n",
              " 3690,\n",
              " 3694,\n",
              " 3695,\n",
              " 3696,\n",
              " 3697,\n",
              " 3698,\n",
              " 3708,\n",
              " 3709,\n",
              " 3710,\n",
              " 3711,\n",
              " 3712,\n",
              " 3718,\n",
              " 3719,\n",
              " 3726,\n",
              " 3729,\n",
              " 3730,\n",
              " 3731,\n",
              " 3732,\n",
              " 3733,\n",
              " 3734,\n",
              " 3737,\n",
              " 3742,\n",
              " 3743,\n",
              " 3744,\n",
              " 3745,\n",
              " 3747,\n",
              " 3750,\n",
              " 3751,\n",
              " 3753,\n",
              " 3754,\n",
              " 3755,\n",
              " 3756,\n",
              " 3779,\n",
              " 3780,\n",
              " 3781,\n",
              " 3784,\n",
              " 3786,\n",
              " 3787,\n",
              " 3794,\n",
              " 3797,\n",
              " 3798,\n",
              " 3799,\n",
              " 3800,\n",
              " 3807,\n",
              " 3831,\n",
              " 3838,\n",
              " 3840,\n",
              " 3847,\n",
              " 3849,\n",
              " 3851,\n",
              " 3852,\n",
              " 3856,\n",
              " 3857,\n",
              " 3860,\n",
              " 3862,\n",
              " 3863,\n",
              " 3869,\n",
              " 3874,\n",
              " 3881,\n",
              " 3883,\n",
              " 3891,\n",
              " 3898,\n",
              " 3900,\n",
              " 3907,\n",
              " 3916,\n",
              " 3931,\n",
              " 3933,\n",
              " 3934,\n",
              " 3948,\n",
              " 3949,\n",
              " 3951,\n",
              " 3952,\n",
              " 3955,\n",
              " 3972,\n",
              " 4013,\n",
              " 4023,\n",
              " 4024,\n",
              " 4032,\n",
              " 4034,\n",
              " 4038,\n",
              " 4039,\n",
              " 4040,\n",
              " 4046,\n",
              " 4074,\n",
              " 4077,\n",
              " 4092,\n",
              " 4095,\n",
              " 4096,\n",
              " 4097,\n",
              " 4105,\n",
              " 4112,\n",
              " 4129,\n",
              " 4130,\n",
              " 4141,\n",
              " 4151,\n",
              " 4152,\n",
              " 4153,\n",
              " 4154,\n",
              " 4155,\n",
              " 4156,\n",
              " 4157,\n",
              " 4158,\n",
              " 4159,\n",
              " 4168,\n",
              " 4171,\n",
              " 4172,\n",
              " 4173,\n",
              " 4175,\n",
              " 4176,\n",
              " 4177,\n",
              " 4253,\n",
              " 4254,\n",
              " 4308,\n",
              " 4315,\n",
              " 4337,\n",
              " 4339,\n",
              " 4340,\n",
              " 4341,\n",
              " 4349,\n",
              " 4354,\n",
              " 4356,\n",
              " 4359,\n",
              " 4396,\n",
              " 4407,\n",
              " 4408,\n",
              " 4411,\n",
              " 4440,\n",
              " 4447,\n",
              " 4448,\n",
              " 4456,\n",
              " 4496,\n",
              " 4514,\n",
              " 4515,\n",
              " 4516,\n",
              " 4530,\n",
              " 4568,\n",
              " 4576,\n",
              " 4585,\n",
              " 4586,\n",
              " 4587,\n",
              " 4588,\n",
              " 4589,\n",
              " 4590,\n",
              " 4591,\n",
              " 4592,\n",
              " 4593,\n",
              " 4620,\n",
              " 4621,\n",
              " 4633,\n",
              " 4634,\n",
              " 4644,\n",
              " 4648,\n",
              " 4652,\n",
              " 4669,\n",
              " 4670,\n",
              " 4745,\n",
              " 4758,\n",
              " 4773,\n",
              " 4787,\n",
              " 4788,\n",
              " 4807,\n",
              " 4828,\n",
              " 4829,\n",
              " 4830,\n",
              " 4845,\n",
              " 4853,\n",
              " 4870,\n",
              " 4929,\n",
              " 4943,\n",
              " 4965,\n",
              " 4975,\n",
              " 4977,\n",
              " 4992,\n",
              " 5029,\n",
              " 5064,\n",
              " 5128,\n",
              " 5182,\n",
              " 5200,\n",
              " 5234,\n",
              " 5241,\n",
              " 5262,\n",
              " 5296,\n",
              " 5336,\n",
              " 5477,\n",
              " 5493,\n",
              " 5531,\n",
              " 5568,\n",
              " 5611,\n",
              " 5684,\n",
              " 5686,\n",
              " 5718,\n",
              " 5742,\n",
              " 5818,\n",
              " 5820,\n",
              " 5829,\n",
              " 5842,\n",
              " 5983,\n",
              " 5989,\n",
              " 6001,\n",
              " 6008,\n",
              " 6110,\n",
              " 6111,\n",
              " 6112,\n",
              " 6113,\n",
              " 6114,\n",
              " 6115,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "best_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXE7AI82Pv7S"
      },
      "outputs": [],
      "source": [
        "best_features.remove(9501)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIwFbXTTQI2T"
      },
      "outputs": [],
      "source": [
        "best_features.remove(9502)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB1ecvcXDfSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d6321-0be3-47e2-be82-b791c7f7514b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":    \n",
        "    \n",
        "   \n",
        "    float_cols=[9501,9502]\n",
        "\n",
        "    X_Data=pd.read_csv(\"output.csv\",header=None,dtype=np.uint16,usecols=best_features)\n",
        "    print(1)\n",
        "    data_2=pd.read_csv(\"output.csv\",header=None,usecols=float_cols)\n",
        "    print(2)\n",
        "    X_Data=clean_dataset(X_Data.iloc[1:,:])\n",
        "    print(3)\n",
        "    data_2=clean_dataset(data_2.iloc[1:,:])\n",
        "    print(4)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o2g0rib48cV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61efc3bd-ec8a-4c8f-830f-3157e05568e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "    data_2=reduce_size_float(data_2)\n",
        "    print(5)\n",
        "    \n",
        "    X_Data=pd.concat([X_Data,data_2],axis=1)\n",
        "    del data_2\n",
        "    print(6)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxY8Z9G5EMcw"
      },
      "outputs": [],
      "source": [
        "y=pd.read_csv(\"output.csv\",usecols=[9505],header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uB2XjR5qENXf",
        "outputId": "9764af2f-2bb4-42be-8758-ed3039c27af3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-aa84dd5e-9f9b-40a4-859c-d1a65b204337\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>9505</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa84dd5e-9f9b-40a4-859c-d1a65b204337')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa84dd5e-9f9b-40a4-859c-d1a65b204337 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa84dd5e-9f9b-40a4-859c-d1a65b204337');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  9505\n",
              "1    2\n",
              "2    2\n",
              "3    2\n",
              "4    2\n",
              "5    2"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "y.iloc[1:,:].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jykRu3GEQnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a54752-5267-4df5-c6a3-8ba08e1b5431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3063: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3041: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_array(key, value)\n"
          ]
        }
      ],
      "source": [
        "y_data=y_labels(y.iloc[1:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-A-HZQzSRYr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8234dd47-1f50-4145-9d40-0554b8c8ce34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-edee2bf1-de13-46e6-8fb1-8de82a36806a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>9505</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edee2bf1-de13-46e6-8fb1-8de82a36806a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-edee2bf1-de13-46e6-8fb1-8de82a36806a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-edee2bf1-de13-46e6-8fb1-8de82a36806a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  9505\n",
              "1    2\n",
              "2    2\n",
              "3    2\n",
              "4    2\n",
              "5    2"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "y_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaX4c-9METjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ca0546-ec64-4ed8-c1dd-65d8e216160c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2        0\n",
              "3        0\n",
              "4        0\n",
              "5        0\n",
              "6        0\n",
              "        ..\n",
              "9498     0\n",
              "9499     0\n",
              "9500     0\n",
              "9501    70\n",
              "9502    70\n",
              "Length: 1238, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "X_Data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Data=X_Data.fillna(0)"
      ],
      "metadata": {
        "id": "AG_Ohvzh5iFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Data.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZgCfvkJ5qnn",
        "outputId": "a2e34130-130b-4809-a0ea-cf102367218b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2       0\n",
              "3       0\n",
              "4       0\n",
              "5       0\n",
              "6       0\n",
              "       ..\n",
              "9498    0\n",
              "9499    0\n",
              "9500    0\n",
              "9501    0\n",
              "9502    0\n",
              "Length: 1238, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_data=reduce_size_float(y_data)"
      ],
      "metadata": {
        "id": "BC1vigI5reQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4bb9b8b-751c-4b05-fb77-c4e187affd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC2fOAxRBlTa"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_Data, y_data, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Fitting Random Forest Classification to the Training set\n",
        "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')"
      ],
      "metadata": {
        "id": "b2DW5ODl3lCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk5hN65m3mzv",
        "outputId": "1c7aa565-e649-4c6b-a158-0d5b7f00bd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(criterion='entropy', n_estimators=10)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "IXOQX7Px3tST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier.score(X_test,Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIdw2t993zCu",
        "outputId": "f52fa308-9aea-4f1e-f860-6769a549f2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9296343951859217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Savefiles(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      self.model.save_weights(\"encoder_model.h5\")\n",
        "      model_json=self.model.to_json()\n",
        "      with open(\"encoder_model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "      !cp encoder_model.json /content/drive/MyDrive/\n",
        "      !cp encoder_model.h5 /content/drive/MyDrive/\n",
        "      !cp history.csv /content/drive/MyDrive/  \n",
        "save_files=Savefiles() "
      ],
      "metadata": {
        "id": "gJ4mPOUkzTjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "0ScVQZbmvf29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUtgDVkOQB7I",
        "outputId": "fd0e66b0-b792-4007-b4ca-f3f52457ae89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(384704, 1238)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 1238\n",
        "\n",
        "# This is the dimension of the latent space (encoding space)\n",
        "latent_dim = 1230\n",
        "\n",
        "encoder = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(latent_dim, activation='relu')\n",
        "])\n",
        "\n",
        "decoder = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(latent_dim,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(input_dim, activation=None)\n",
        "])"
      ],
      "metadata": {
        "id": "nAErHIlkOtM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Model(inputs=encoder.input, outputs=decoder(encoder.output))"
      ],
      "metadata": {
        "id": "gRVql-JCO0IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NzHls7W4Qlpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = tf.keras.optimizers.Adam(lr = 0.01, epsilon = 0.1)\n",
        "autoencoder.compile(optimizer = adam, \n",
        "                  loss='mse'\n",
        "                 )\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_loss',\n",
        "                              mode='min', \n",
        "                              verbose=1, \n",
        "                              patience=20\n",
        "                             )\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              mode='min',\n",
        "                              verbose=1,\n",
        "                              patience=10,\n",
        "                              min_delta=0.0001,\n",
        "                              factor=0.2\n",
        "                             )\n",
        "\n",
        "filename='history.csv'\n",
        "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)"
      ],
      "metadata": {
        "id": "N8Vq2CRoysvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d56a972-267d-469a-ecf1-0d7aa17f2eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=K.constant(X_train)\n",
        "X_test=K.constant(X_test)"
      ],
      "metadata": {
        "id": "noAj4m1J3WOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zac27VARAas9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(X_train, X_train, epochs=1000, batch_size=16, validation_data=(X_test,X_test),callbacks = [history_logger,save_files, earlystopping, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA89HVkswFDc",
        "outputId": "9ab7dbf9-d8c2-43f3-bcb1-ddb59c6c7ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.8852 - val_loss: 1.0063 - lr: 0.0100\n",
            "Epoch 2/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.8805 - val_loss: 0.9987 - lr: 0.0100\n",
            "Epoch 3/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.8796 - val_loss: 1.0011 - lr: 0.0100\n",
            "Epoch 4/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.8789 - val_loss: 1.0018 - lr: 0.0100\n",
            "Epoch 5/1000\n",
            "19236/19236 [==============================] - 146s 8ms/step - loss: 0.8773 - val_loss: 1.0538 - lr: 0.0100\n",
            "Epoch 6/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.9138 - val_loss: 1.0384 - lr: 0.0100\n",
            "Epoch 7/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.9243 - val_loss: 1.0367 - lr: 0.0100\n",
            "Epoch 8/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.9189 - val_loss: 1.0377 - lr: 0.0100\n",
            "Epoch 9/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.9038 - val_loss: 1.0322 - lr: 0.0100\n",
            "Epoch 10/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.9183 - val_loss: 1.0467 - lr: 0.0100\n",
            "Epoch 11/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.9187 - val_loss: 1.0299 - lr: 0.0100\n",
            "Epoch 12/1000\n",
            "19232/19236 [============================>.] - ETA: 0s - loss: 0.9070\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.9069 - val_loss: 1.0396 - lr: 0.0100\n",
            "Epoch 13/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.9041 - val_loss: 1.0227 - lr: 0.0020\n",
            "Epoch 14/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.8875 - val_loss: 0.9844 - lr: 0.0020\n",
            "Epoch 15/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.8593 - val_loss: 0.9714 - lr: 0.0020\n",
            "Epoch 16/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.8503 - val_loss: 0.9601 - lr: 0.0020\n",
            "Epoch 17/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.8472 - val_loss: 0.9587 - lr: 0.0020\n",
            "Epoch 18/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.8402 - val_loss: 0.9463 - lr: 0.0020\n",
            "Epoch 19/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.8356 - val_loss: 0.9387 - lr: 0.0020\n",
            "Epoch 20/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.8290 - val_loss: 0.9428 - lr: 0.0020\n",
            "Epoch 21/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.8225 - val_loss: 0.9342 - lr: 0.0020\n",
            "Epoch 22/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.8114 - val_loss: 0.9180 - lr: 0.0020\n",
            "Epoch 23/1000\n",
            "19236/19236 [==============================] - 153s 8ms/step - loss: 0.8050 - val_loss: 0.9192 - lr: 0.0020\n",
            "Epoch 24/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.7980 - val_loss: 0.9103 - lr: 0.0020\n",
            "Epoch 25/1000\n",
            "19236/19236 [==============================] - 153s 8ms/step - loss: 0.7932 - val_loss: 0.9120 - lr: 0.0020\n",
            "Epoch 26/1000\n",
            "19236/19236 [==============================] - 153s 8ms/step - loss: 0.7919 - val_loss: 0.9092 - lr: 0.0020\n",
            "Epoch 27/1000\n",
            "19236/19236 [==============================] - 153s 8ms/step - loss: 0.7914 - val_loss: 0.9065 - lr: 0.0020\n",
            "Epoch 28/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.7861 - val_loss: 0.9005 - lr: 0.0020\n",
            "Epoch 29/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.7856 - val_loss: 0.8977 - lr: 0.0020\n",
            "Epoch 30/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.7835 - val_loss: 0.8943 - lr: 0.0020\n",
            "Epoch 31/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.7832 - val_loss: 0.8963 - lr: 0.0020\n",
            "Epoch 32/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.7800 - val_loss: 0.8882 - lr: 0.0020\n",
            "Epoch 33/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.7758 - val_loss: 0.8916 - lr: 0.0020\n",
            "Epoch 34/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.7758 - val_loss: 0.8919 - lr: 0.0020\n",
            "Epoch 35/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.7728 - val_loss: 0.8814 - lr: 0.0020\n",
            "Epoch 36/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.7744 - val_loss: 0.8884 - lr: 0.0020\n",
            "Epoch 37/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.7724 - val_loss: 0.8807 - lr: 0.0020\n",
            "Epoch 38/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.7676 - val_loss: 0.8777 - lr: 0.0020\n",
            "Epoch 39/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.7662 - val_loss: 0.8740 - lr: 0.0020\n",
            "Epoch 40/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.7639 - val_loss: 0.8728 - lr: 0.0020\n",
            "Epoch 41/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.7617 - val_loss: 0.8693 - lr: 0.0020\n",
            "Epoch 42/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.7592 - val_loss: 0.8689 - lr: 0.0020\n",
            "Epoch 43/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.7576 - val_loss: 0.8670 - lr: 0.0020\n",
            "Epoch 44/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.7586 - val_loss: 0.8605 - lr: 0.0020\n",
            "Epoch 45/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.7542 - val_loss: 0.8628 - lr: 0.0020\n",
            "Epoch 46/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.7519 - val_loss: 0.8640 - lr: 0.0020\n",
            "Epoch 47/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.7515 - val_loss: 0.8569 - lr: 0.0020\n",
            "Epoch 48/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.7502 - val_loss: 0.8595 - lr: 0.0020\n",
            "Epoch 49/1000\n",
            "19236/19236 [==============================] - 155s 8ms/step - loss: 0.7488 - val_loss: 0.8555 - lr: 0.0020\n",
            "Epoch 50/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.7479 - val_loss: 0.8565 - lr: 0.0020\n",
            "Epoch 51/1000\n",
            "19236/19236 [==============================] - 142s 7ms/step - loss: 0.7474 - val_loss: 0.8569 - lr: 0.0020\n",
            "Epoch 52/1000\n",
            "19236/19236 [==============================] - 143s 7ms/step - loss: 0.7461 - val_loss: 0.8552 - lr: 0.0020\n",
            "Epoch 53/1000\n",
            "19236/19236 [==============================] - 143s 7ms/step - loss: 0.7476 - val_loss: 0.8552 - lr: 0.0020\n",
            "Epoch 54/1000\n",
            "19236/19236 [==============================] - 142s 7ms/step - loss: 0.7456 - val_loss: 0.8523 - lr: 0.0020\n",
            "Epoch 55/1000\n",
            "19236/19236 [==============================] - 144s 8ms/step - loss: 0.7440 - val_loss: 0.8499 - lr: 0.0020\n",
            "Epoch 56/1000\n",
            "19236/19236 [==============================] - 142s 7ms/step - loss: 0.7433 - val_loss: 0.8707 - lr: 0.0020\n",
            "Epoch 57/1000\n",
            "19236/19236 [==============================] - 143s 7ms/step - loss: 0.7572 - val_loss: 0.8695 - lr: 0.0020\n",
            "Epoch 58/1000\n",
            "19236/19236 [==============================] - 144s 7ms/step - loss: 0.7513 - val_loss: 0.8544 - lr: 0.0020\n",
            "Epoch 59/1000\n",
            "19236/19236 [==============================] - 143s 7ms/step - loss: 0.7389 - val_loss: 0.8422 - lr: 0.0020\n",
            "Epoch 60/1000\n",
            "19236/19236 [==============================] - 141s 7ms/step - loss: 0.7335 - val_loss: 0.8377 - lr: 0.0020\n",
            "Epoch 61/1000\n",
            "19236/19236 [==============================] - 141s 7ms/step - loss: 0.7308 - val_loss: 0.8438 - lr: 0.0020\n",
            "Epoch 62/1000\n",
            "19236/19236 [==============================] - 143s 7ms/step - loss: 0.7293 - val_loss: 0.8383 - lr: 0.0020\n",
            "Epoch 63/1000\n",
            "19236/19236 [==============================] - 140s 7ms/step - loss: 0.7276 - val_loss: 0.8365 - lr: 0.0020\n",
            "Epoch 64/1000\n",
            "19236/19236 [==============================] - 141s 7ms/step - loss: 0.7255 - val_loss: 0.8377 - lr: 0.0020\n",
            "Epoch 65/1000\n",
            "19236/19236 [==============================] - 144s 8ms/step - loss: 0.7248 - val_loss: 0.8349 - lr: 0.0020\n",
            "Epoch 66/1000\n",
            "19236/19236 [==============================] - 141s 7ms/step - loss: 0.7223 - val_loss: 0.8333 - lr: 0.0020\n",
            "Epoch 67/1000\n",
            "19236/19236 [==============================] - 142s 7ms/step - loss: 0.7198 - val_loss: 0.8335 - lr: 0.0020\n",
            "Epoch 68/1000\n",
            "19236/19236 [==============================] - 144s 7ms/step - loss: 0.7199 - val_loss: 0.8302 - lr: 0.0020\n",
            "Epoch 69/1000\n",
            "19236/19236 [==============================] - 142s 7ms/step - loss: 0.7172 - val_loss: 0.8275 - lr: 0.0020\n",
            "Epoch 70/1000\n",
            "19236/19236 [==============================] - 144s 8ms/step - loss: 0.7162 - val_loss: 0.8271 - lr: 0.0020\n",
            "Epoch 71/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.7148 - val_loss: 0.8237 - lr: 0.0020\n",
            "Epoch 72/1000\n",
            "19236/19236 [==============================] - 156s 8ms/step - loss: 0.7123 - val_loss: 0.8222 - lr: 0.0020\n",
            "Epoch 73/1000\n",
            "19236/19236 [==============================] - 156s 8ms/step - loss: 0.7095 - val_loss: 0.8215 - lr: 0.0020\n",
            "Epoch 74/1000\n",
            "19236/19236 [==============================] - 159s 8ms/step - loss: 0.7088 - val_loss: 0.8242 - lr: 0.0020\n",
            "Epoch 75/1000\n",
            "19236/19236 [==============================] - 155s 8ms/step - loss: 0.7078 - val_loss: 0.8215 - lr: 0.0020\n",
            "Epoch 76/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.7055 - val_loss: 0.8221 - lr: 0.0020\n",
            "Epoch 77/1000\n",
            "19236/19236 [==============================] - 155s 8ms/step - loss: 0.7045 - val_loss: 0.8146 - lr: 0.0020\n",
            "Epoch 78/1000\n",
            "19236/19236 [==============================] - 155s 8ms/step - loss: 0.7031 - val_loss: 0.8133 - lr: 0.0020\n",
            "Epoch 79/1000\n",
            "19236/19236 [==============================] - 158s 8ms/step - loss: 0.7026 - val_loss: 0.8124 - lr: 0.0020\n",
            "Epoch 80/1000\n",
            "19236/19236 [==============================] - 156s 8ms/step - loss: 0.7000 - val_loss: 0.8112 - lr: 0.0020\n",
            "Epoch 81/1000\n",
            "19236/19236 [==============================] - 160s 8ms/step - loss: 0.6967 - val_loss: 0.8053 - lr: 0.0020\n",
            "Epoch 82/1000\n",
            "19236/19236 [==============================] - 160s 8ms/step - loss: 0.6951 - val_loss: 0.8065 - lr: 0.0020\n",
            "Epoch 83/1000\n",
            "19236/19236 [==============================] - 160s 8ms/step - loss: 0.6939 - val_loss: 0.8042 - lr: 0.0020\n",
            "Epoch 84/1000\n",
            "19236/19236 [==============================] - 160s 8ms/step - loss: 0.6941 - val_loss: 0.8057 - lr: 0.0020\n",
            "Epoch 85/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6945 - val_loss: 0.8059 - lr: 0.0020\n",
            "Epoch 86/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.6927 - val_loss: 0.8043 - lr: 0.0020\n",
            "Epoch 87/1000\n",
            "19236/19236 [==============================] - 160s 8ms/step - loss: 0.6908 - val_loss: 0.7980 - lr: 0.0020\n",
            "Epoch 88/1000\n",
            "19236/19236 [==============================] - 162s 8ms/step - loss: 0.6880 - val_loss: 0.7956 - lr: 0.0020\n",
            "Epoch 89/1000\n",
            "19236/19236 [==============================] - 157s 8ms/step - loss: 0.6860 - val_loss: 0.7986 - lr: 0.0020\n",
            "Epoch 90/1000\n",
            "19236/19236 [==============================] - 157s 8ms/step - loss: 0.6843 - val_loss: 0.7970 - lr: 0.0020\n",
            "Epoch 91/1000\n",
            "19236/19236 [==============================] - 155s 8ms/step - loss: 0.6825 - val_loss: 0.7994 - lr: 0.0020\n",
            "Epoch 92/1000\n",
            "19236/19236 [==============================] - 156s 8ms/step - loss: 0.6830 - val_loss: 0.7927 - lr: 0.0020\n",
            "Epoch 93/1000\n",
            "19236/19236 [==============================] - 157s 8ms/step - loss: 0.6833 - val_loss: 0.7921 - lr: 0.0020\n",
            "Epoch 94/1000\n",
            "19236/19236 [==============================] - 156s 8ms/step - loss: 0.6815 - val_loss: 0.7929 - lr: 0.0020\n",
            "Epoch 95/1000\n",
            "19236/19236 [==============================] - 153s 8ms/step - loss: 0.6800 - val_loss: 0.7893 - lr: 0.0020\n",
            "Epoch 96/1000\n",
            "19236/19236 [==============================] - 153s 8ms/step - loss: 0.6754 - val_loss: 0.7881 - lr: 0.0020\n",
            "Epoch 97/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.6734 - val_loss: 0.7893 - lr: 0.0020\n",
            "Epoch 98/1000\n",
            "19236/19236 [==============================] - 155s 8ms/step - loss: 0.6706 - val_loss: 0.7825 - lr: 0.0020\n",
            "Epoch 99/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.6696 - val_loss: 0.7811 - lr: 0.0020\n",
            "Epoch 100/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.6671 - val_loss: 0.7761 - lr: 0.0020\n",
            "Epoch 101/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.6675 - val_loss: 0.7789 - lr: 0.0020\n",
            "Epoch 102/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6648 - val_loss: 0.7759 - lr: 0.0020\n",
            "Epoch 103/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.6629 - val_loss: 0.7674 - lr: 0.0020\n",
            "Epoch 104/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.6599 - val_loss: 0.7706 - lr: 0.0020\n",
            "Epoch 105/1000\n",
            "19236/19236 [==============================] - 156s 8ms/step - loss: 0.6582 - val_loss: 0.7608 - lr: 0.0020\n",
            "Epoch 106/1000\n",
            "19236/19236 [==============================] - 157s 8ms/step - loss: 0.6562 - val_loss: 0.7685 - lr: 0.0020\n",
            "Epoch 107/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.6587 - val_loss: 0.7677 - lr: 0.0020\n",
            "Epoch 108/1000\n",
            "19236/19236 [==============================] - 153s 8ms/step - loss: 0.6527 - val_loss: 0.7586 - lr: 0.0020\n",
            "Epoch 109/1000\n",
            "19236/19236 [==============================] - 155s 8ms/step - loss: 0.6523 - val_loss: 0.7536 - lr: 0.0020\n",
            "Epoch 110/1000\n",
            "19236/19236 [==============================] - 156s 8ms/step - loss: 0.6497 - val_loss: 0.7560 - lr: 0.0020\n",
            "Epoch 111/1000\n",
            "19236/19236 [==============================] - 158s 8ms/step - loss: 0.6481 - val_loss: 0.7554 - lr: 0.0020\n",
            "Epoch 112/1000\n",
            "19236/19236 [==============================] - 154s 8ms/step - loss: 0.6460 - val_loss: 0.7493 - lr: 0.0020\n",
            "Epoch 113/1000\n",
            "19236/19236 [==============================] - 151s 8ms/step - loss: 0.6448 - val_loss: 0.7521 - lr: 0.0020\n",
            "Epoch 114/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.6463 - val_loss: 0.7565 - lr: 0.0020\n",
            "Epoch 115/1000\n",
            "19236/19236 [==============================] - 145s 8ms/step - loss: 0.6453 - val_loss: 0.7500 - lr: 0.0020\n",
            "Epoch 116/1000\n",
            "19236/19236 [==============================] - 146s 8ms/step - loss: 0.6436 - val_loss: 0.7507 - lr: 0.0020\n",
            "Epoch 117/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6424 - val_loss: 0.7481 - lr: 0.0020\n",
            "Epoch 118/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6406 - val_loss: 0.7519 - lr: 0.0020\n",
            "Epoch 119/1000\n",
            "19236/19236 [==============================] - 146s 8ms/step - loss: 0.6417 - val_loss: 0.7462 - lr: 0.0020\n",
            "Epoch 120/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.6374 - val_loss: 0.7625 - lr: 0.0020\n",
            "Epoch 121/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6372 - val_loss: 0.7480 - lr: 0.0020\n",
            "Epoch 122/1000\n",
            "19236/19236 [==============================] - 146s 8ms/step - loss: 0.6358 - val_loss: 0.7443 - lr: 0.0020\n",
            "Epoch 123/1000\n",
            "19236/19236 [==============================] - 145s 8ms/step - loss: 0.6343 - val_loss: 0.7427 - lr: 0.0020\n",
            "Epoch 124/1000\n",
            "19236/19236 [==============================] - 145s 8ms/step - loss: 0.6363 - val_loss: 0.7432 - lr: 0.0020\n",
            "Epoch 125/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6356 - val_loss: 0.7417 - lr: 0.0020\n",
            "Epoch 126/1000\n",
            "19236/19236 [==============================] - 145s 8ms/step - loss: 0.6329 - val_loss: 0.7430 - lr: 0.0020\n",
            "Epoch 127/1000\n",
            "19236/19236 [==============================] - 144s 7ms/step - loss: 0.6340 - val_loss: 0.7414 - lr: 0.0020\n",
            "Epoch 128/1000\n",
            "19236/19236 [==============================] - 144s 8ms/step - loss: 0.6374 - val_loss: 0.7419 - lr: 0.0020\n",
            "Epoch 129/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6342 - val_loss: 0.7403 - lr: 0.0020\n",
            "Epoch 130/1000\n",
            "19236/19236 [==============================] - 146s 8ms/step - loss: 0.6314 - val_loss: 0.7398 - lr: 0.0020\n",
            "Epoch 131/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6290 - val_loss: 0.7459 - lr: 0.0020\n",
            "Epoch 132/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.6300 - val_loss: 0.7341 - lr: 0.0020\n",
            "Epoch 133/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6291 - val_loss: 0.7365 - lr: 0.0020\n",
            "Epoch 134/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6267 - val_loss: 0.7303 - lr: 0.0020\n",
            "Epoch 135/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6248 - val_loss: 0.7328 - lr: 0.0020\n",
            "Epoch 136/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.6242 - val_loss: 0.7337 - lr: 0.0020\n",
            "Epoch 137/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6235 - val_loss: 0.7295 - lr: 0.0020\n",
            "Epoch 138/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.6215 - val_loss: 0.7296 - lr: 0.0020\n",
            "Epoch 139/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6221 - val_loss: 0.7280 - lr: 0.0020\n",
            "Epoch 140/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.6206 - val_loss: 0.7289 - lr: 0.0020\n",
            "Epoch 141/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6209 - val_loss: 0.7238 - lr: 0.0020\n",
            "Epoch 142/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6200 - val_loss: 0.7253 - lr: 0.0020\n",
            "Epoch 143/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6198 - val_loss: 0.7294 - lr: 0.0020\n",
            "Epoch 144/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.6191 - val_loss: 0.7239 - lr: 0.0020\n",
            "Epoch 145/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6252 - val_loss: 0.7290 - lr: 0.0020\n",
            "Epoch 146/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6237 - val_loss: 0.7317 - lr: 0.0020\n",
            "Epoch 147/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.6195 - val_loss: 0.7233 - lr: 0.0020\n",
            "Epoch 148/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6178 - val_loss: 0.7260 - lr: 0.0020\n",
            "Epoch 149/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6181 - val_loss: 0.7274 - lr: 0.0020\n",
            "Epoch 150/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6171 - val_loss: 0.7209 - lr: 0.0020\n",
            "Epoch 151/1000\n",
            "19236/19236 [==============================] - 147s 8ms/step - loss: 0.6154 - val_loss: 0.7198 - lr: 0.0020\n",
            "Epoch 152/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6144 - val_loss: 0.7235 - lr: 0.0020\n",
            "Epoch 153/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6139 - val_loss: 0.7241 - lr: 0.0020\n",
            "Epoch 154/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6122 - val_loss: 0.7196 - lr: 0.0020\n",
            "Epoch 155/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6128 - val_loss: 0.7192 - lr: 0.0020\n",
            "Epoch 156/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6107 - val_loss: 0.7182 - lr: 0.0020\n",
            "Epoch 157/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6102 - val_loss: 0.7206 - lr: 0.0020\n",
            "Epoch 158/1000\n",
            "19236/19236 [==============================] - 152s 8ms/step - loss: 0.6103 - val_loss: 0.7195 - lr: 0.0020\n",
            "Epoch 159/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6093 - val_loss: 0.7169 - lr: 0.0020\n",
            "Epoch 160/1000\n",
            "19236/19236 [==============================] - 148s 8ms/step - loss: 0.6077 - val_loss: 0.7131 - lr: 0.0020\n",
            "Epoch 161/1000\n",
            "19236/19236 [==============================] - 160s 8ms/step - loss: 0.6067 - val_loss: 0.7148 - lr: 0.0020\n",
            "Epoch 162/1000\n",
            "19236/19236 [==============================] - 150s 8ms/step - loss: 0.6063 - val_loss: 0.7097 - lr: 0.0020\n",
            "Epoch 163/1000\n",
            "19236/19236 [==============================] - 149s 8ms/step - loss: 0.6050 - val_loss: 0.7139 - lr: 0.0020\n",
            "Epoch 164/1000\n",
            "17969/19236 [===========================>..] - ETA: 9s - loss: 0.6054"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Model(inputs=visible, outputs=bottleneck)"
      ],
      "metadata": {
        "id": "JCiroksOwPF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.save('encoder.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxBAKqhSAKKN",
        "outputId": "342f29fd-0ab8-4cb6-9ce8-6be1cd251373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "HMegEnxVVsdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = load_model('encoder.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7rU1Op1V0Wu",
        "outputId": "b3cd4e80-a603-4971-ad5f-8868d22cb843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of final_data_preprocessing_and_auto_encoding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}