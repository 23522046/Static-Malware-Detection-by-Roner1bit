# -*- coding: utf-8 -*-
"""Copy of final_data_preprocessing_and_auto_encoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xw3y6cHvD-hlmsWSwbrVrrpjIdeujfbV
"""

!gdown --id 170-6j9q2YmCT7tmwo8tgP3D0jjH489lA
!unzip output.zip
!mv /content/content/output.csv /content/output.csv

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense
import random
from sklearn.preprocessing import StandardScaler

import numpy as np
import pandas as pd
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
import tensorflow as tf
from keras.models import model_from_json
from tensorflow.keras.models import Model, load_model, save_model
import keras
from keras import backend as K

from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

def clean_dataset(df):
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep]

def y_labels(data):
    dict={'Adware':0,'Banker':1 , 'FileInfector':2,'PUA':3,'Riskware':4,'SMS':5,'Trojan':6,'Backdoor':7,'Dropper':8,'NoCategory':9,'Ransomware':10, 'Scareware':11 ,'Spy':12 ,'Zeroday':13,'Benign':14}
    for key, value in dict.items():
      data[data[9505]==key]=value
    return data

def reduce_size_float(data):
  for col in data.columns:
    data[col] = data[col].astype(np.int16)
  return data

best_features=[]
myFile = open("best_features.txt", "r")
for myLine in myFile:
  try:
    best_features.append(int(myLine))
  except:
    continue

best_features

len(best_features)

best_features.sort()

best_features

best_features.remove(9501)

best_features.remove(9502)

if __name__ == "__main__":    
    
   
    float_cols=[9501,9502]

    X_Data=pd.read_csv("output.csv",header=None,dtype=np.uint16,usecols=best_features)
    print(1)
    data_2=pd.read_csv("output.csv",header=None,usecols=float_cols)
    print(2)
    X_Data=clean_dataset(X_Data.iloc[1:,:])
    print(3)
    data_2=clean_dataset(data_2.iloc[1:,:])
    print(4)

data_2=reduce_size_float(data_2)
    print(5)
    
    X_Data=pd.concat([X_Data,data_2],axis=1)
    del data_2
    print(6)

y=pd.read_csv("output.csv",usecols=[9505],header=None)

y.iloc[1:,:].head()

y_data=y_labels(y.iloc[1:,:])

y_data.head()

X_Data.isna().sum()

X_Data=X_Data.fillna(0)

X_Data.isna().sum()

y_data=reduce_size_float(y_data)

X_train, X_test, Y_train, Y_test = train_test_split(X_Data, y_data, test_size=0.20, random_state=42)

from sklearn.ensemble import RandomForestClassifier
# Fitting Random Forest Classification to the Training set
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')

classifier.fit(X_train, Y_train)

y_pred=classifier.predict(X_test)

print(classifier.score(X_test,Y_test))

class Savefiles(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
      self.model.save_weights("encoder_model.h5")
      model_json=self.model.to_json()
      with open("encoder_model.json", "w") as json_file:
        json_file.write(model_json)
      !cp encoder_model.json /content/drive/MyDrive/
      !cp encoder_model.h5 /content/drive/MyDrive/
      !cp history.csv /content/drive/MyDrive/  
save_files=Savefiles()

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

X_Data.shape

input_dim = 1238

# This is the dimension of the latent space (encoding space)
latent_dim = 1230

encoder = Sequential([
    Dense(128, activation='relu', input_shape=(input_dim,)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(latent_dim, activation='relu')
])

decoder = Sequential([
    Dense(64, activation='relu', input_shape=(latent_dim,)),
    Dense(128, activation='relu'),
    Dense(256, activation='relu'),
    Dense(input_dim, activation=None)
])

autoencoder = Model(inputs=encoder.input, outputs=decoder(encoder.output))



adam = tf.keras.optimizers.Adam(lr = 0.01, epsilon = 0.1)
autoencoder.compile(optimizer = adam, 
                  loss='mse'
                 )

earlystopping = EarlyStopping(monitor='val_loss',
                              mode='min', 
                              verbose=1, 
                              patience=20
                             )

reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              mode='min',
                              verbose=1,
                              patience=10,
                              min_delta=0.0001,
                              factor=0.2
                             )

filename='history.csv'
history_logger=tf.keras.callbacks.CSVLogger(filename, separator=",", append=True)

X_train=K.constant(X_train)
X_test=K.constant(X_test)



history = autoencoder.fit(X_train, X_train, epochs=1000, batch_size=16, validation_data=(X_test,X_test),callbacks = [history_logger,save_files, earlystopping, reduce_lr])

encoder = Model(inputs=visible, outputs=bottleneck)

encoder.save('encoder.h5')

from tensorflow.keras.models import load_model

encoder = load_model('encoder.h5')